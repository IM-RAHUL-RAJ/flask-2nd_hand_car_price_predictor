# -*- coding: utf-8 -*-
"""car_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17e5GwakIL_kPbR1ytp25TV9Yunh252XE
"""



import pandas as pd

df=pd.read_csv('car data.csv')

df.head()

df.shape

df['Seller_Type'].unique()

df['Transmission'].unique()

df['Owner'].unique()

#find missing values
df.isnull()

df.isnull().sum()

df.describe()

df.describe().sum()

df.head()

df.columns

df=df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',
       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]

df

df.head()

#no of years old
df['Year']=2020-df['Year']

df.head()

df=pd.get_dummies(df)

df.head()

df.corr()

import seaborn as sns

sns.pairplot(df)

import matplotlib.pyplot as plt

df.index

df.corr().index

cormat=df.corr()
top_corr_features=cormat.index
plt.figure(figsize=(20,20))
hm=sns.heatmap(df.corr(),annot=True,cmap='RdYlGn')

df.head()

df['years_old']=df['Year']

df.head()

df.drop('Year',axis=1,inplace=True)

df.head()

x=df.iloc[:,1:]
y=df.iloc[:,0]

x

y

x.head()

y.head()

#feature scaling

from sklearn.ensemble import ExtraTreesRegressor
model=ExtraTreesRegressor()
model.fit(x,y)

print(model.feature_importances_)

feat_imp=pd.Series(model.feature_importances_,index=x.columns)
feat_imp.nlargest(5).plot(kind='barh')
plt.show()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)

x_train.shape

from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor()

#hyperparameters
import numpy as np
n_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]

print(n_estimators)

from sklearn.model_selection import RandomizedSearchCV

#Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

print(random_grid)

rf=RandomForestRegressor()

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42,n_jobs=1)

rf_random.fit(x_train,y_train)

predictions=rf_random.predict(x_test)

predictions

df.head()

#diff btw y_test and predicted values
sns.distplot(y_test-predictions)

plt.scatter(y_test,predictions,color="red")
plt.scatter(y_test,y_test)

import pickle
file=open('car_price_prediction.pkl','wb')
pickle.dump(rf_random,file)

